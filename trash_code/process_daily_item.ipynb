{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = get_mongodb_client()\n",
    "RAW_DATABASE = 'scrapy'\n",
    "DES_DATABASE = 'real_estate'\n",
    "\n",
    "ITEM_COLLECTION = 'item'\n",
    "NEWS_COLLECTION = 'new'\n",
    "\n",
    "PROJECT_COLLECTION = 'project'\n",
    "COMMUNE_ADDRESS_COLLECTION = 'commune_address'\n",
    "BASEPROJECT_COLLECTION = 'base_project'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_collection = client[RAW_DATABASE][ITEM_COLLECTION]\n",
    "news_collection = client[DES_DATABASE][NEWS_COLLECTION]\n",
    "\n",
    "project_collection = client[DES_DATABASE][PROJECT_COLLECTION]\n",
    "commune_address_collection = client[DES_DATABASE][COMMUNE_ADDRESS_COLLECTION]\n",
    "base_project_collection = client[DES_DATABASE][BASEPROJECT_COLLECTION]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get latest date process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_createdAt = news_collection.find_one({}, sort=[(\"createdAt\", -1)])[\"createdAt\"]\n",
    "max_process_id = news_collection.find_one({}, sort=[(\"process_id\", -1)])[\"process_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_process_id = max_process_id + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data not process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "item_response = item_collection.find({'createdAt': {\n",
    "    \"$gt\": max_createdAt\n",
    "}})\n",
    "full_raw_data = list(item_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "687"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Project data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects_response = project_collection.find({})\n",
    "full_project_df = pd.DataFrame(list(projects_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove missing address data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "687\n"
     ]
    }
   ],
   "source": [
    "trash_data = []\n",
    "used_data = []\n",
    "\n",
    "\n",
    "for news in full_raw_data:\n",
    "    if ('project' in news and check_valid_string_field(news['project'])) or ('district' in news and check_valid_string_field(news['district'])):\n",
    "        used_data.append(news)\n",
    "    else:\n",
    "        news['reason'] = \"Can't find address from this item\"\n",
    "        trash_data.append(news)\n",
    "\n",
    "print(len(used_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove missing price or square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_data_no_missing = []\n",
    "unused_data = []\n",
    "for item in used_data:\n",
    "    if 'total_price' not in item or 'square' not in item or 'price_per_m2' not in item:\n",
    "        print(item)\n",
    "        item['reason'] = 'Missing price'\n",
    "        unused_data.append(item)\n",
    "        continue\n",
    "    price = item['total_price']\n",
    "    square = item['square']\n",
    "    price_per_m2 = item['price_per_m2']\n",
    "\n",
    "    if price == 0 or price is None or square == 0 or square is None or price_per_m2 == 0 or price_per_m2 is None:\n",
    "        item['reason'] = 'Missing price'\n",
    "        unused_data.append(item)\n",
    "        continue\n",
    "    used_data_no_missing.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "669"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(used_data_no_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "price_per_m2_values = np.array([d['price_per_m2'] for d in used_data_no_missing])\n",
    "square_values = np.array([d['square'] for d in used_data_no_missing])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "656"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "used_data_no_outlier = [d for d in used_data_no_missing if 3e6 < d['price_per_m2'] < 3e8 and d['square'] > 20]\n",
    "len(used_data_no_outlier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 656/656 [00:00<00:00, 5702.10it/s]\n"
     ]
    }
   ],
   "source": [
    "from unidecode import unidecode\n",
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "duplicate_item = []\n",
    "tokens = [\n",
    "    unidecode(d['title']).strip().lower().split() for d in used_data_no_outlier\n",
    "]\n",
    "smoothing_function = SmoothingFunction().method2\n",
    "list_unique_item = []\n",
    "processed_item = set()\n",
    "for i in tqdm(range(len(used_data_no_outlier))):\n",
    "    item = used_data_no_outlier[i]\n",
    "    if i in processed_item:\n",
    "        continue\n",
    "    processed_item.add(i)\n",
    "    unique_item = item\n",
    "\n",
    "    reference_tokens = [tokens[i]]\n",
    "    for j in range(len(used_data_no_outlier)):\n",
    "        if j in processed_item:\n",
    "            continue\n",
    "        another_item = used_data_no_outlier[j]\n",
    "        candidate_tokens = tokens[j]\n",
    "        if item['square'] == another_item['square'] and is_approximately(unique_item['total_price'], another_item['total_price']):\n",
    "            if sentence_bleu(reference_tokens, candidate_tokens, weights=(0.5, 0.5), smoothing_function=smoothing_function) > 0.9:\n",
    "                processed_item.add(j)\n",
    "                duplicate_item.append([unique_item['news_url'], another_item['news_url']])\n",
    "                if unique_item['published_at'] < another_item['published_at']:\n",
    "                    unique_item = another_item\n",
    "    list_unique_item.append(unique_item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "629"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_unique_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare to old news in database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "prev_date = datetime.now() - timedelta(days=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_news = list(news_collection.find({\n",
    "    'published_at': {\n",
    "        '$gt': prev_date\n",
    "    }\n",
    "    },\n",
    "    {\n",
    "        'title': 1,\n",
    "        'square': 1,\n",
    "        'total_price': 1\n",
    "    }\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 629/629 [00:03<00:00, 157.38it/s]\n"
     ]
    }
   ],
   "source": [
    "old_tokens = [\n",
    "    d['title'].split() for d in old_news\n",
    "]\n",
    "\n",
    "list_unique_item_from_old = []\n",
    "for i in tqdm(range(len(list_unique_item))):\n",
    "    item = list_unique_item[i]\n",
    "\n",
    "    reference_tokens = [tokens[i]]\n",
    "    is_duplicate = False\n",
    "    for j in range(len(old_news)):\n",
    "        another_item = old_news[j]\n",
    "        candidate_tokens = old_tokens[j]\n",
    "        if item['square'] == another_item['square']:\n",
    "            if sentence_bleu(reference_tokens, candidate_tokens, weights=(0.5, 0.5), smoothing_function=smoothing_function) > 0.9:\n",
    "                if item['published_at'] - timedelta(days=15) <  another_item['published_at']:\n",
    "                    is_duplicate = True\n",
    "                    break\n",
    "    if not is_duplicate:\n",
    "        list_unique_item_from_old.append(item)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "629"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_unique_item_from_old)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoongParser:\n",
    "    _instance = None\n",
    "    _connection = None\n",
    "\n",
    "    def __new__(cls, *args, **kwargs):\n",
    "        if not cls._instance:\n",
    "            cls._instance = super(GoongParser, cls).__new__(cls, *args, **kwargs)\n",
    "        return cls._instance\n",
    "\n",
    "    def __init__(self):\n",
    "        if not self._connection:\n",
    "            self._connection = self._connect_to_server()\n",
    "\n",
    "    def _connect_to_server(self):\n",
    "        # Implement your connection logic here\n",
    "        # This could involve creating a network connection, authenticating, etc.\n",
    "        conn = http.client.HTTPSConnection('rsapi.goong.io')\n",
    "        return conn\n",
    "\n",
    "    def _reconnect(self):\n",
    "        self._connection = self._connect_to_server()\n",
    "        \n",
    "    def parse_address(self, address):\n",
    "        params = urllib.parse.urlencode({\n",
    "            'address': address,\n",
    "            'api_key': config.get(GOONG_CONGIG_NAME, 'key')\n",
    "        })\n",
    "        # Implement the geocoding logic using the server connection\n",
    "        self._connection.request('GET', '/Geocode?{}'.format(params))\n",
    "\n",
    "        res = self._connection.getresponse()\n",
    "        data = res.read()\n",
    "        return json.loads(data)\n",
    "\n",
    "    def get_connection(self):\n",
    "        return self._connection\n",
    "    \n",
    "    def finalize(self):\n",
    "        # Implement any cleanup logic here (e.g., closing the connection)\n",
    "        self._connection.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_domain = {\n",
    "    'alonhadat': 'https://alonhadat.com.vn/',\n",
    "    'homedy': 'https://homedy.com/'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_of_project = ['căn hộ chung cư', 'khu biệt thự', 'nhà phố', 'khu phức hợp', 'căn hộ dịch vụ', 'khu nghỉ dưỡng', 'cao ốc văn phòng', 'khu thương mại', 'khu dân cư', 'nhà ở xã hội', 'khu đô thị mới', 'khu tái định cư', 'khu đô thị', 'tòa nhà']\n",
    "\n",
    "def extract_name_project(project: str):\n",
    "    project = project.strip().lower()\n",
    "    for kind in type_of_project:\n",
    "        project = project.replace(kind, '')\n",
    "    project = project.strip().title()\n",
    "    return project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loc_by_address(address):\n",
    "    res = list(commune_address_collection.find({'formatted_address': address}))\n",
    "    if len(res) > 0:\n",
    "        return res[0]['loc']\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_project = list(full_project_df['formatted_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_project = set()\n",
    "for news in tqdm(list_unique_item_from_old):\n",
    "    if 'project' in news and check_valid_string_field(news['project']):\n",
    "        name = news['project']\n",
    "        formatted_address = get_formatted_address(news['commune'], news['district'], news['province'])\n",
    "\n",
    "        url = None\n",
    "        if 'project_url' in news and news['project_url'] is not None:\n",
    "            domain = list_domain[news['source']]\n",
    "            url =  get_absolute_path(news['project_url'], domain)\n",
    "\n",
    "        reduced_name = extract_name_project(name)\n",
    "        if get_formatted_string(reduced_name) not in formatted_address:\n",
    "            name = reduced_name\n",
    "        match_project = full_project_df[full_project_df['formatted_name'] == get_formatted_string(name)]\n",
    "\n",
    "        if len(match_project) == 0:\n",
    "            query = ', '.join([name, news['district'], news['province']])\n",
    "            unseen_project.add((name, news['source'],url, query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "list_unseen_project = []\n",
    "parser = GoongParser()\n",
    "i = 0\n",
    "for prj in unseen_project:\n",
    "    print(i)\n",
    "    i += 1\n",
    "    new_prj = {}\n",
    "    new_prj['name'] = prj[0]\n",
    "    new_prj['source'] = prj[1]\n",
    "    new_prj['url'] = prj[2]\n",
    "    new_prj['formatted_name'] = get_formatted_string(prj[0])\n",
    "\n",
    "    try:\n",
    "        res = parser.parse_address(prj[3])\n",
    "        new_prj['parser_response'] = res\n",
    "    except:\n",
    "        parser._reconnect()\n",
    "        res = parser.parse_address(prj[3])\n",
    "        new_prj['parser_response'] = res\n",
    "    time.sleep(1.5)\n",
    "    list_unseen_project.append(new_prj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_new_project = []\n",
    "list_match_project = []\n",
    "for new_prj in list_unseen_project:\n",
    "    name = new_prj['name']\n",
    "    print(name)\n",
    "    new_prj['loc'] = {\n",
    "        'type': 'Point',\n",
    "        'coordinates': [\n",
    "            new_prj['parser_response']['results'][0]['geometry']['location']['lng'],\n",
    "            new_prj['parser_response']['results'][0]['geometry']['location']['lat'],\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    new_prj['address'] = {\n",
    "        'name':  new_prj['parser_response']['results'][0]['name'],\n",
    "        'address': new_prj['parser_response']['results'][0]['address'],\n",
    "        'compound':  new_prj['parser_response']['results'][0]['compound'],\n",
    "        'formatted_compound': get_formatted_compound(new_prj['parser_response']['results'][0]['compound'])\n",
    "    }\n",
    "    is_match = False\n",
    "    for id, row in full_project_df.iterrows():\n",
    "        if string_equal(name, row['name']):\n",
    "            if calculate_distance(new_prj['loc'], row['loc']) < 0.5:\n",
    "                print(calculate_distance(new_prj['loc'], row['loc']))\n",
    "                new_prj['base_project'] = row['base_project']\n",
    "                list_match_project.append(new_prj)\n",
    "                is_match = True\n",
    "                break\n",
    "    if not is_match:\n",
    "        list_new_project.append(new_prj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x7f4c1f1b79d0>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_collection.insert_many(list_match_project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_project_collection = client['real_estate']['base_project']\n",
    "result = base_project_collection.find_one({}, sort=[('project_id', -1)], projection={'project_id': 1})\n",
    "max_project_id = result['project_id']\n",
    "\n",
    "i = 1\n",
    "for base in list_new_project:\n",
    "    base['project_id'] = max_project_id + i\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x7f4c492298e0>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_project_collection.insert_many(list_new_project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "for new_prj in list_new_project:\n",
    "    prj = {\n",
    "        'name': new_prj['name'],\n",
    "        'source': new_prj['source'],\n",
    "        'url': new_prj['url'],\n",
    "        'loc': new_prj['loc'],\n",
    "        'parser_response': new_prj['parser_response'],\n",
    "        'address': new_prj['address'],\n",
    "        'formatted_name': new_prj['formatted_name'],\n",
    "        'base_project': {\n",
    "            'project_id': new_prj['project_id'],\n",
    "            'name': new_prj['name'],\n",
    "            'address': new_prj['address'],\n",
    "            'loc': new_prj['loc'],\n",
    "            'url': new_prj['url'],\n",
    "            'source': new_prj['source']\n",
    "        }\n",
    "    }\n",
    "    project_collection.insert_one(prj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects_response = project_collection.find({})\n",
    "full_project_df = pd.DataFrame(list(projects_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▎   | 400/629 [00:19<00:03, 69.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Address not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 629/629 [00:24<00:00, 25.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "used_data_with_loc = []\n",
    "commune_address_collection = client['real_estate']['commune_address']\n",
    "for news in tqdm(list_unique_item_from_old):\n",
    "    if 'project' in news and check_valid_string_field(news['project']):\n",
    "        name = news['project']\n",
    "        reduced_name = extract_name_project(name)\n",
    "        formatted_address = get_formatted_address(news['commune'], news['district'], news['province'])\n",
    "        if get_formatted_string(reduced_name) not in formatted_address:\n",
    "            name = reduced_name\n",
    "        \n",
    "        name = get_formatted_string(name)\n",
    "\n",
    "        match_project = full_project_df[full_project_df['formatted_name'] == name]\n",
    "\n",
    "        if len(match_project) == 0:\n",
    "            print('Have project but not found')\n",
    "            continue\n",
    "        \n",
    "        news['loc'] = match_project.iloc[0]['loc']\n",
    "        news['base_project'] = match_project.iloc[0]['base_project']\n",
    "        news['location_confidence'] = 1\n",
    "    else:\n",
    "        news['project'] = None\n",
    "        news['project_url'] = None\n",
    "        formatted_address = get_formatted_address(news['commune'], news['district'], news['province'])\n",
    "        query = {\n",
    "            'formatted_address': formatted_address\n",
    "        }\n",
    "\n",
    "        res = list(commune_address_collection.find(query))\n",
    "        if len(res) == 0:\n",
    "            print('Address not found')\n",
    "            continue\n",
    "        else:\n",
    "            news['loc'] = res[0]['loc']\n",
    "            \n",
    "        if news['commune']:\n",
    "            news['location_confidence'] = 0\n",
    "        else:\n",
    "            news['location_confidence'] = -1\n",
    "            query_ = {\n",
    "                'formatted_compound.district': unidecode(news['district']).strip().lower(),\n",
    "                'formatted_compound.province': unidecode(news['province']).strip().lower()\n",
    "            }   \n",
    "            res_ = list(commune_address_collection.find(query_))\n",
    "            news['list_commune_match'] = []\n",
    "            for loc in res_:\n",
    "                news['list_commune_match'].append(loc['loc'])\n",
    "    \n",
    "    used_data_with_loc.append(news)\n",
    "print(len(used_data_with_loc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "for i in range(len(used_data_with_loc)):\n",
    "    new = used_data_with_loc[i]\n",
    "    new['process_id'] = this_process_id\n",
    "    new['transformAt'] = datetime.now() \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_coordinates(coord):\n",
    "    latitude = coord[1]\n",
    "    longitude = coord[0]\n",
    "    if latitude < -90 or latitude > 90:\n",
    "        return False\n",
    "    \n",
    "    if longitude < -180 or longitude > 180:\n",
    "        return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "for new in used_data_with_loc:\n",
    "    coordinates = new['loc']['coordinates']\n",
    "    if not is_valid_coordinates(coordinates):\n",
    "        print(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x7f4c02c7ad90>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_collection = client['real_estate']['new']\n",
    "new_collection.insert_many(used_data_with_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "process-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
